{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6dfe870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "YOLOv8 Fine-tuning Script for Enemy Detection\n",
    "\"\"\"\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dcfa4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enemy_detection_model():\n",
    "    \"\"\"\n",
    "    Fine-tune YOLOv8 model for enemy detection using augmented data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    DATA_YAML = \"augmented_data/dataset.yaml\"\n",
    "    PRETRAINED_MODEL = \"yolov8n.pt\"  # or yolov8s.pt, yolov8m.pt, yolov8l.pt, yolov8x.pt\n",
    "    PROJECT_NAME = \"enemy_detection\"\n",
    "    EXPERIMENT_NAME = \"run1\"\n",
    "    \n",
    "    # Training parameters\n",
    "    EPOCHS = 100\n",
    "    BATCH_SIZE = 16  # Adjust based on your GPU memory\n",
    "    IMAGE_SIZE = 640\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"YOLOv8 Enemy Detection Fine-tuning\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if not os.path.exists(DATA_YAML):\n",
    "        print(f\"âŒ Dataset file not found: {DATA_YAML}\")\n",
    "        print(\"Please run the data augmentation notebook first!\")\n",
    "        return\n",
    "    \n",
    "    # Load pretrained model\n",
    "    print(f\"ğŸ“¦ Loading pretrained model: {PRETRAINED_MODEL}\")\n",
    "    model = YOLO(PRETRAINED_MODEL)\n",
    "    \n",
    "    # Display model info\n",
    "    print(f\"ğŸ“Š Model architecture: {PRETRAINED_MODEL}\")\n",
    "    print(f\"ğŸ“ Dataset: {DATA_YAML}\")\n",
    "    print(f\"ğŸ¯ Target: Enemy detection (1 class)\")\n",
    "    print(f\"âš™ï¸  Epochs: {EPOCHS}\")\n",
    "    print(f\"ğŸ“ Image size: {IMAGE_SIZE}\")\n",
    "    print(f\"ğŸ”¢ Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    print(\"\\nğŸš€ Starting training...\")\n",
    "    \n",
    "    # Train the model\n",
    "    results = model.train(\n",
    "        data=DATA_YAML,\n",
    "        epochs=EPOCHS,\n",
    "        imgsz=IMAGE_SIZE,\n",
    "        batch=BATCH_SIZE,\n",
    "        project=PROJECT_NAME,\n",
    "        name=EXPERIMENT_NAME,\n",
    "        save=True,\n",
    "        save_period=10,  # Save checkpoint every 10 epochs\n",
    "        patience=50,     # Early stopping patience\n",
    "        device=0,        # Use GPU 0, change to 'cpu' if no GPU\n",
    "        workers=8,       # Number of worker threads\n",
    "        cache=True,      # Cache images for faster training\n",
    "        # Data augmentation (YOLOv8 handles this automatically)\n",
    "        hsv_h=0.015,     # Image HSV-Hue augmentation\n",
    "        hsv_s=0.7,       # Image HSV-Saturation augmentation\n",
    "        hsv_v=0.4,       # Image HSV-Value augmentation\n",
    "        degrees=0.0,     # Image rotation (+/- deg)\n",
    "        translate=0.1,   # Image translation (+/- fraction)\n",
    "        scale=0.5,       # Image scale (+/- gain)\n",
    "        shear=0.0,       # Image shear (+/- deg)\n",
    "        perspective=0.0, # Image perspective (+/- fraction)\n",
    "        flipud=0.0,      # Image flip up-down (probability)\n",
    "        fliplr=0.5,      # Image flip left-right (probability)\n",
    "        mosaic=1.0,      # Image mosaic (probability)\n",
    "        mixup=0.0,       # Image mixup (probability)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… Training completed!\")\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"ğŸ“ Results saved to: {PROJECT_NAME}/{EXPERIMENT_NAME}\")\n",
    "    print(f\"ğŸ† Best model: {PROJECT_NAME}/{EXPERIMENT_NAME}/weights/best.pt\")\n",
    "    print(f\"ğŸ“ˆ Training plots: {PROJECT_NAME}/{EXPERIMENT_NAME}/\")\n",
    "    \n",
    "    # Validate the model\n",
    "    print(\"\\nğŸ” Running validation...\")\n",
    "    metrics = model.val()\n",
    "    \n",
    "    print(\"\\nğŸ“Š Validation Metrics:\")\n",
    "    print(f\"   mAP50: {metrics.box.map50:.3f}\")\n",
    "    print(f\"   mAP50-95: {metrics.box.map:.3f}\")\n",
    "    print(f\"   Precision: {metrics.box.mp:.3f}\")\n",
    "    print(f\"   Recall: {metrics.box.mr:.3f}\")\n",
    "    \n",
    "    return model, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fbc0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_person_detection_model():\n",
    "    \"\"\"\n",
    "    Fine-tune YOLOv8 model to detect game characters as 'person' class\n",
    "    This leverages pretrained knowledge for faster, better training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration - Use existing augmented data\n",
    "    DATA_YAML = \"augmented_data/dataset.yaml\"  # Your existing data\n",
    "    PRETRAINED_MODEL = \"yolov8n.pt\"  \n",
    "    PROJECT_NAME = \"person_detection\"\n",
    "    EXPERIMENT_NAME = \"game_characters\"\n",
    "    \n",
    "    # Training parameters (much fewer epochs needed!)\n",
    "    EPOCHS = 25  # Reduced from 100 since we're fine-tuning existing person class\n",
    "    BATCH_SIZE = 16\n",
    "    IMAGE_SIZE = 640\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"YOLOv8 Person Detection Fine-tuning (Game Characters)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if not os.path.exists(DATA_YAML):\n",
    "        print(f\"âŒ Dataset file not found: {DATA_YAML}\")\n",
    "        print(\"Please run the data augmentation notebook first!\")\n",
    "        return\n",
    "    \n",
    "    # Load pretrained model\n",
    "    print(f\"ğŸ“¦ Loading pretrained model: {PRETRAINED_MODEL}\")\n",
    "    model = YOLO(PRETRAINED_MODEL)\n",
    "    \n",
    "    # Display model info\n",
    "    print(f\"ğŸ“Š Model architecture: {PRETRAINED_MODEL}\")\n",
    "    print(f\"ğŸ“ Dataset: {DATA_YAML}\")\n",
    "    print(f\"ğŸ¯ Target: Person detection (leveraging pretrained class)\")\n",
    "    print(f\"âš™ï¸  Epochs: {EPOCHS} (reduced due to transfer learning)\")\n",
    "    print(f\"ğŸ“ Image size: {IMAGE_SIZE}\")\n",
    "    print(f\"ğŸ”¢ Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"ğŸ’¡ Strategy: Game characters â†’ Person class (ID 0)\")\n",
    "    \n",
    "    print(\"\\nğŸš€ Starting fine-tuning...\")\n",
    "    \n",
    "    # Train the model with transfer learning settings\n",
    "    results = model.train(\n",
    "        data=DATA_YAML,\n",
    "        epochs=EPOCHS,\n",
    "        imgsz=IMAGE_SIZE,\n",
    "        batch=BATCH_SIZE,\n",
    "        project=PROJECT_NAME,\n",
    "        name=EXPERIMENT_NAME,\n",
    "        save=True,\n",
    "        save_period=5,   # Save more frequently\n",
    "        patience=15,     # Earlier stopping\n",
    "        device=0,        # Use GPU 0, 'cpu' if no GPU\n",
    "        workers=8,\n",
    "        cache=True,\n",
    "        # Optimized settings for fine-tuning person detection\n",
    "        lr0=0.001,       # Lower initial learning rate for fine-tuning\n",
    "        lrf=0.1,         # Learning rate final multiplier\n",
    "        warmup_epochs=3, # Warmup epochs\n",
    "        # Conservative augmentation for stability\n",
    "        hsv_h=0.01,      # Minimal hue shift\n",
    "        hsv_s=0.4,       # Moderate saturation\n",
    "        hsv_v=0.3,       # Moderate value changes\n",
    "        degrees=5.0,     # Small rotation\n",
    "        translate=0.05,  # Small translation\n",
    "        scale=0.2,       # Small scale changes\n",
    "        fliplr=0.5,      # Horizontal flip OK\n",
    "        mosaic=0.8,      # Reduced mosaic\n",
    "        mixup=0.1,       # Light mixup\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… Fine-tuning completed!\")\n",
    "    print(f\"ğŸ“ Results saved to: {PROJECT_NAME}/{EXPERIMENT_NAME}\")\n",
    "    print(f\"ğŸ† Best model: {PROJECT_NAME}/{EXPERIMENT_NAME}/weights/best.pt\")\n",
    "    \n",
    "    # Validate\n",
    "    print(\"\\nğŸ” Running validation...\")\n",
    "    metrics = model.val()\n",
    "    \n",
    "    print(\"\\nğŸ“Š Validation Metrics:\")\n",
    "    print(f\"   mAP50: {metrics.box.map50:.3f}\")\n",
    "    print(f\"   mAP50-95: {metrics.box.map:.3f}\")\n",
    "    print(f\"   Precision: {metrics.box.mp:.3f}\")\n",
    "    print(f\"   Recall: {metrics.box.mr:.3f}\")\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ef40ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def split_dataset(train_ratio=0.8, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Split the augmented dataset into training and validation sets\n",
    "    \n",
    "    Args:\n",
    "        train_ratio: Ratio of data for training (0.8 = 80%)\n",
    "        val_ratio: Ratio of data for validation (0.2 = 20%)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ğŸ“Š Splitting dataset into train/validation sets...\")\n",
    "    \n",
    "    # Paths\n",
    "    data_dir = Path(\"augmented_data\")\n",
    "    images_dir = data_dir\n",
    "    labels_dir = data_dir / \"labels\"\n",
    "    \n",
    "    # Create train/val directories\n",
    "    train_images_dir = data_dir / \"train\" / \"images\"\n",
    "    train_labels_dir = data_dir / \"train\" / \"labels\"\n",
    "    val_images_dir = data_dir / \"val\" / \"images\"\n",
    "    val_labels_dir = data_dir / \"val\" / \"labels\"\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_path in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.png\"))\n",
    "    image_files = [f for f in image_files if f.is_file() and not f.parent.name in ['train', 'val']]\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to split\")\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(\"âŒ No images found! Make sure you've run the data augmentation first.\")\n",
    "        return False\n",
    "    \n",
    "    # Shuffle the files\n",
    "    random.shuffle(image_files)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    train_count = int(len(image_files) * train_ratio)\n",
    "    \n",
    "    train_files = image_files[:train_count]\n",
    "    val_files = image_files[train_count:]\n",
    "    \n",
    "    print(f\"ğŸ“ Training set: {len(train_files)} images ({len(train_files)/len(image_files)*100:.1f}%)\")\n",
    "    print(f\"ğŸ“ Validation set: {len(val_files)} images ({len(val_files)/len(image_files)*100:.1f}%)\")\n",
    "    \n",
    "    # Copy files to train directory\n",
    "    for img_file in train_files:\n",
    "        # Copy image\n",
    "        shutil.copy2(img_file, train_images_dir / img_file.name)\n",
    "        \n",
    "        # Copy corresponding label\n",
    "        label_file = labels_dir / (img_file.stem + \".txt\")\n",
    "        if label_file.exists():\n",
    "            shutil.copy2(label_file, train_labels_dir / label_file.name)\n",
    "    \n",
    "    # Copy files to validation directory\n",
    "    for img_file in val_files:\n",
    "        # Copy image\n",
    "        shutil.copy2(img_file, val_images_dir / img_file.name)\n",
    "        \n",
    "        # Copy corresponding label\n",
    "        label_file = labels_dir / (img_file.stem + \".txt\")\n",
    "        if label_file.exists():\n",
    "            shutil.copy2(label_file, val_labels_dir / label_file.name)\n",
    "    \n",
    "    # Create updated dataset.yaml\n",
    "    dataset_yaml_content = f\"\"\"path: {data_dir.absolute()}\n",
    "train: train/images\n",
    "val: val/images\n",
    "\n",
    "nc: 1\n",
    "names:\n",
    "  0: person\n",
    "\"\"\"\n",
    "    \n",
    "    with open(data_dir / \"dataset_split.yaml\", 'w') as f:\n",
    "        f.write(dataset_yaml_content)\n",
    "    \n",
    "    print(\"âœ… Dataset split completed!\")\n",
    "    print(f\"ğŸ“ Created dataset_split.yaml with train/val paths\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def train_person_detection_model():\n",
    "    \"\"\"\n",
    "    Fine-tune YOLOv8 model to detect game characters as 'person' class\n",
    "    This leverages pretrained knowledge for faster, better training\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, split the dataset\n",
    "    print(\"ğŸ”„ Preparing dataset...\")\n",
    "    if not split_dataset(train_ratio=0.8, val_ratio=0.2):\n",
    "        return None, None\n",
    "    \n",
    "    # Configuration - Use split dataset\n",
    "    DATA_YAML = \"augmented_data/dataset_split.yaml\"  # Use split dataset\n",
    "    PRETRAINED_MODEL = \"yolov8n.pt\"  \n",
    "    PROJECT_NAME = \"person_detection\"\n",
    "    EXPERIMENT_NAME = \"game_characters\"\n",
    "    \n",
    "    # Training parameters (much fewer epochs needed!)\n",
    "    EPOCHS = 25  # Reduced from 100 since we're fine-tuning existing person class\n",
    "    BATCH_SIZE = 16\n",
    "    IMAGE_SIZE = 640\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"YOLOv8 Person Detection Fine-tuning (Game Characters)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if not os.path.exists(DATA_YAML):\n",
    "        print(f\"âŒ Dataset file not found: {DATA_YAML}\")\n",
    "        print(\"Please run the data augmentation notebook first!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load pretrained model\n",
    "    print(f\"ğŸ“¦ Loading pretrained model: {PRETRAINED_MODEL}\")\n",
    "    model = YOLO(PRETRAINED_MODEL)\n",
    "    \n",
    "    # Display model info\n",
    "    print(f\"ğŸ“Š Model architecture: {PRETRAINED_MODEL}\")\n",
    "    print(f\"ğŸ“ Dataset: {DATA_YAML}\")\n",
    "    print(f\"ğŸ¯ Target: Person detection (leveraging pretrained class)\")\n",
    "    print(f\"âš™ï¸  Epochs: {EPOCHS} (reduced due to transfer learning)\")\n",
    "    print(f\"ğŸ“ Image size: {IMAGE_SIZE}\")\n",
    "    print(f\"ğŸ”¢ Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"ğŸ’¡ Strategy: Game characters â†’ Person class (ID 0)\")\n",
    "    print(f\"ğŸ“Š Data split: 80% train, 20% validation\")\n",
    "    \n",
    "    print(\"\\nğŸš€ Starting fine-tuning...\")\n",
    "    \n",
    "    # Train the model with transfer learning settings\n",
    "    results = model.train(\n",
    "        data=DATA_YAML,\n",
    "        epochs=EPOCHS,\n",
    "        imgsz=IMAGE_SIZE,\n",
    "        batch=BATCH_SIZE,\n",
    "        project=PROJECT_NAME,\n",
    "        name=EXPERIMENT_NAME,\n",
    "        save=True,\n",
    "        save_period=5,   # Save more frequently\n",
    "        patience=15,     # Earlier stopping\n",
    "        device=0,        # Use GPU 0, 'cpu' if no GPU\n",
    "        workers=8,\n",
    "        cache=True,\n",
    "        # Optimized settings for fine-tuning person detection\n",
    "        lr0=0.001,       # Lower initial learning rate for fine-tuning\n",
    "        lrf=0.1,         # Learning rate final multiplier\n",
    "        warmup_epochs=3, # Warmup epochs\n",
    "        # Conservative augmentation for stability\n",
    "        hsv_h=0.01,      # Minimal hue shift\n",
    "        hsv_s=0.4,       # Moderate saturation\n",
    "        hsv_v=0.3,       # Moderate value changes\n",
    "        degrees=5.0,     # Small rotation\n",
    "        translate=0.05,  # Small translation\n",
    "        scale=0.2,       # Small scale changes\n",
    "        fliplr=0.5,      # Horizontal flip OK\n",
    "        mosaic=0.8,      # Reduced mosaic\n",
    "        mixup=0.1,       # Light mixup\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… Fine-tuning completed!\")\n",
    "    print(f\"ğŸ“ Results saved to: {PROJECT_NAME}/{EXPERIMENT_NAME}\")\n",
    "    print(f\"ğŸ† Best model: {PROJECT_NAME}/{EXPERIMENT_NAME}/weights/best.pt\")\n",
    "    \n",
    "    # Validate on the validation set\n",
    "    print(\"\\nğŸ” Running validation on held-out validation set...\")\n",
    "    metrics = model.val()\n",
    "    \n",
    "    print(\"\\nğŸ“Š Final Validation Metrics:\")\n",
    "    print(f\"   mAP50: {metrics.box.map50:.3f}\")\n",
    "    print(f\"   mAP50-95: {metrics.box.map:.3f}\")\n",
    "    print(f\"   Precision: {metrics.box.mp:.3f}\")\n",
    "    print(f\"   Recall: {metrics.box.mr:.3f}\")\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a4cb87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def save_detection_visualizations(model, output_dir=\"detection_outputs\", max_images=20):\n",
    "    \"\"\"\n",
    "    Save images with bounding boxes drawn around detected characters\n",
    "    \n",
    "    Args:\n",
    "        model: Trained YOLO model\n",
    "        output_dir: Directory to save visualization images\n",
    "        max_images: Maximum number of images to process\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"ğŸ’¾ Saving detection visualizations to: {output_dir}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get validation images\n",
    "    val_images_dir = Path(\"augmented_data/val/images\")\n",
    "    if not val_images_dir.exists():\n",
    "        print(\"âŒ No validation images found! Make sure you've split the dataset.\")\n",
    "        return\n",
    "    \n",
    "    val_images = list(val_images_dir.glob(\"*.jpg\"))[:max_images]\n",
    "    \n",
    "    if len(val_images) == 0:\n",
    "        print(\"âŒ No validation images found!\")\n",
    "        return\n",
    "    \n",
    "    detection_summary = []\n",
    "    \n",
    "    for i, img_path in enumerate(val_images):\n",
    "        print(f\"Processing image {i+1}/{len(val_images)}: {img_path.name}\")\n",
    "        \n",
    "        # Run inference\n",
    "        results = model(str(img_path), conf=0.3, save=False, verbose=False)\n",
    "        \n",
    "        # Load original image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        original_height, original_width = img_rgb.shape[:2]\n",
    "        \n",
    "        # Draw detections\n",
    "        detections_count = 0\n",
    "        high_conf_count = 0\n",
    "        \n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                    # Get box coordinates and confidence\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                    conf = box.conf[0].cpu().numpy()\n",
    "                    \n",
    "                    detections_count += 1\n",
    "                    if conf > 0.7:\n",
    "                        high_conf_count += 1\n",
    "                    \n",
    "                    # Choose color based on confidence\n",
    "                    if conf > 0.8:\n",
    "                        color = (0, 255, 0)  # Bright green for very high confidence\n",
    "                        thickness = 3\n",
    "                    elif conf > 0.5:\n",
    "                        color = (255, 165, 0)  # Orange for medium confidence\n",
    "                        thickness = 2\n",
    "                    else:\n",
    "                        color = (255, 255, 0)  # Yellow for low confidence\n",
    "                        thickness = 2\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(img_rgb, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness)\n",
    "                    \n",
    "                    # Calculate box size for text scaling\n",
    "                    box_width = x2 - x1\n",
    "                    font_scale = min(max(box_width / 200, 0.4), 1.0)  # Scale font with box size\n",
    "                    \n",
    "                    # Add confidence label\n",
    "                    label = f\"Person {conf:.2f}\"\n",
    "                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 2)[0]\n",
    "                    \n",
    "                    # Draw label background\n",
    "                    cv2.rectangle(img_rgb, \n",
    "                                (int(x1), int(y1) - label_size[1] - 10), \n",
    "                                (int(x1) + label_size[0], int(y1)), \n",
    "                                color, -1)\n",
    "                    \n",
    "                    # Draw label text\n",
    "                    cv2.putText(img_rgb, label, (int(x1), int(y1) - 5), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), 2)\n",
    "        \n",
    "        # Add image info text at top\n",
    "        info_text = f\"Detections: {detections_count} | High Conf (>0.7): {high_conf_count}\"\n",
    "        cv2.putText(img_rgb, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        cv2.putText(img_rgb, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 1)\n",
    "        \n",
    "        # Save the image\n",
    "        output_filename = f\"detection_{i+1:03d}_{img_path.stem}_conf{detections_count}.jpg\"\n",
    "        output_file_path = output_path / output_filename\n",
    "        \n",
    "        # Convert back to BGR for saving\n",
    "        img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(str(output_file_path), img_bgr)\n",
    "        \n",
    "        # Store summary info\n",
    "        detection_summary.append({\n",
    "            'image': img_path.name,\n",
    "            'total_detections': detections_count,\n",
    "            'high_confidence': high_conf_count,\n",
    "            'output_file': output_filename\n",
    "        })\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_path = output_path / \"detection_summary.txt\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"DETECTION VISUALIZATION SUMMARY\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total images processed: {len(val_images)}\\n\")\n",
    "        f.write(f\"Output directory: {output_dir}\\n\\n\")\n",
    "        \n",
    "        f.write(\"COLOR CODING:\\n\")\n",
    "        f.write(\"ğŸŸ¢ Bright Green: High confidence (>0.8)\\n\")\n",
    "        f.write(\"ğŸŸ  Orange: Medium confidence (0.5-0.8)\\n\")\n",
    "        f.write(\"ğŸŸ¡ Yellow: Low confidence (<0.5)\\n\\n\")\n",
    "        \n",
    "        f.write(\"IMAGE DETAILS:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        \n",
    "        total_detections = 0\n",
    "        total_high_conf = 0\n",
    "        \n",
    "        for item in detection_summary:\n",
    "            f.write(f\"File: {item['output_file']}\\n\")\n",
    "            f.write(f\"  Original: {item['image']}\\n\")\n",
    "            f.write(f\"  Total detections: {item['total_detections']}\\n\")\n",
    "            f.write(f\"  High confidence: {item['high_confidence']}\\n\")\n",
    "            f.write(f\"  Detection rate: {item['total_detections']/1:.1f} per image\\n\\n\")\n",
    "            \n",
    "            total_detections += item['total_detections']\n",
    "            total_high_conf += item['high_confidence']\n",
    "        \n",
    "        f.write(\"OVERALL STATISTICS:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(f\"Average detections per image: {total_detections/len(val_images):.1f}\\n\")\n",
    "        f.write(f\"Average high-confidence per image: {total_high_conf/len(val_images):.1f}\\n\")\n",
    "        f.write(f\"High-confidence rate: {(total_high_conf/total_detections*100) if total_detections > 0 else 0:.1f}%\\n\")\n",
    "    \n",
    "    print(f\"\\nâœ… Saved {len(val_images)} visualization images\")\n",
    "    print(f\"ğŸ“„ Summary report: {summary_path}\")\n",
    "    print(f\"ğŸ¨ Color coding: Green (high conf) | Orange (medium) | Yellow (low)\")\n",
    "    \n",
    "    return detection_summary\n",
    "\n",
    "def comprehensive_model_evaluation():\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the trained model with detailed metrics and visualizations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the best trained model\n",
    "    model_path = \"person_detection/game_characters/weights/best.pt\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"âŒ Trained model not found: {model_path}\")\n",
    "        print(\"Please run training first!\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ§ª Starting comprehensive model evaluation...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # 1. Save detection visualizations FIRST\n",
    "    print(\"ğŸ¨ 1. SAVING DETECTION VISUALIZATIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    detection_output_dir = f\"detection_outputs_{timestamp}\"\n",
    "    \n",
    "    detection_summary = save_detection_visualizations(model, detection_output_dir, max_images=25)\n",
    "    \n",
    "    # 2. Detailed validation metrics\n",
    "    print(f\"\\nğŸ“Š 2. DETAILED VALIDATION METRICS\")\n",
    "    print(\"-\" * 40)\n",
    "    metrics = model.val()\n",
    "    \n",
    "    # Create detailed metrics report\n",
    "    metrics_report = {\n",
    "        'mAP50': metrics.box.map50,\n",
    "        'mAP50-95': metrics.box.map,\n",
    "        'Precision': metrics.box.mp,\n",
    "        'Recall': metrics.box.mr,\n",
    "        'F1-Score': 2 * (metrics.box.mp * metrics.box.mr) / (metrics.box.mp + metrics.box.mr) if (metrics.box.mp + metrics.box.mr) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"ğŸ¯ mAP@0.5: {metrics_report['mAP50']:.3f}\")\n",
    "    print(f\"ğŸ¯ mAP@0.5:0.95: {metrics_report['mAP50-95']:.3f}\")\n",
    "    print(f\"ğŸ¯ Precision: {metrics_report['Precision']:.3f}\")\n",
    "    print(f\"ğŸ¯ Recall: {metrics_report['Recall']:.3f}\")\n",
    "    print(f\"ğŸ¯ F1-Score: {metrics_report['F1-Score']:.3f}\")\n",
    "    \n",
    "    # 3. Display sample visualizations in notebook\n",
    "    print(f\"\\nğŸ–¼ï¸  3. SAMPLE VISUALIZATIONS IN NOTEBOOK\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    val_images_dir = Path(\"augmented_data/val/images\")\n",
    "    if val_images_dir.exists():\n",
    "        val_images = list(val_images_dir.glob(\"*.jpg\"))[:6]  # Show 6 images in notebook\n",
    "        \n",
    "        if len(val_images) > 0:\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            detection_stats = {'total_detections': 0, 'high_conf_detections': 0}\n",
    "            \n",
    "            for i, img_path in enumerate(val_images):\n",
    "                # Run inference\n",
    "                results = model(str(img_path), conf=0.3)  # Lower confidence for testing\n",
    "                \n",
    "                # Load and process image\n",
    "                img = cv2.imread(str(img_path))\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Draw detections\n",
    "                for r in results:\n",
    "                    boxes = r.boxes\n",
    "                    if boxes is not None:\n",
    "                        for box in boxes:\n",
    "                            # Get box coordinates\n",
    "                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                            conf = box.conf[0].cpu().numpy()\n",
    "                            \n",
    "                            detection_stats['total_detections'] += 1\n",
    "                            if conf > 0.7:\n",
    "                                detection_stats['high_conf_detections'] += 1\n",
    "                            \n",
    "                            # Draw rectangle\n",
    "                            color = (0, 255, 0) if conf > 0.7 else (255, 165, 0)  # Green for high conf, orange for low\n",
    "                            cv2.rectangle(img_rgb, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "                            \n",
    "                            # Add label\n",
    "                            label = f\"Person {conf:.2f}\"\n",
    "                            cv2.putText(img_rgb, label, (int(x1), int(y1)-10), \n",
    "                                      cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                \n",
    "                # Display\n",
    "                axes[i].imshow(img_rgb)\n",
    "                axes[i].set_title(f\"Val Image {i+1}\\nDetections: {len(results[0].boxes) if results[0].boxes is not None else 0}\")\n",
    "                axes[i].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(\"Model Performance on Validation Set\\n(Green: High Confidence >0.7, Orange: Low Confidence)\", y=1.02)\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"ğŸ“ˆ Detection Statistics (notebook samples):\")\n",
    "            print(f\"   Total detections: {detection_stats['total_detections']}\")\n",
    "            print(f\"   High confidence (>0.7): {detection_stats['high_conf_detections']}\")\n",
    "            print(f\"   Detection rate: {detection_stats['high_conf_detections']/len(val_images):.1f} per image\")\n",
    "        \n",
    "        else:\n",
    "            print(\"âŒ No validation images found!\")\n",
    "    \n",
    "    # 4. Confidence threshold analysis\n",
    "    print(f\"\\nğŸ“Š 4. CONFIDENCE THRESHOLD ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if val_images_dir.exists():\n",
    "        thresholds = [0.3, 0.5, 0.7, 0.9]\n",
    "        threshold_results = {}\n",
    "        \n",
    "        test_images = list(val_images_dir.glob(\"*.jpg\"))[:10]  # Test on 10 images\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            total_detections = 0\n",
    "            for img_path in test_images:\n",
    "                results = model(str(img_path), conf=thresh)\n",
    "                for r in results:\n",
    "                    if r.boxes is not None:\n",
    "                        total_detections += len(r.boxes)\n",
    "            \n",
    "            avg_detections = total_detections / len(test_images)\n",
    "            threshold_results[thresh] = avg_detections\n",
    "            print(f\"   Confidence {thresh}: {avg_detections:.1f} avg detections per image\")\n",
    "    \n",
    "    # 5. Performance timing test\n",
    "    print(f\"\\nâ±ï¸  5. PERFORMANCE TIMING TEST\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if len(val_images) > 0:\n",
    "        import time\n",
    "        \n",
    "        # Warm up\n",
    "        _ = model(str(val_images[0]))\n",
    "        \n",
    "        # Time multiple inferences\n",
    "        times = []\n",
    "        for i in range(10):\n",
    "            start_time = time.time()\n",
    "            _ = model(str(val_images[i % len(val_images)]))\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        fps = 1.0 / avg_time\n",
    "        \n",
    "        print(f\"ğŸš€ Average inference time: {avg_time*1000:.1f}ms\")\n",
    "        print(f\"ğŸš€ Estimated FPS: {fps:.1f}\")\n",
    "        print(f\"ğŸš€ Real-time capable: {'âœ… Yes' if fps >= 30 else 'âŒ No (for 30+ FPS)'}\")\n",
    "    \n",
    "    # 6. Save comprehensive report\n",
    "    print(f\"\\nğŸ’¾ 6. SAVING EVALUATION REPORT\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create detailed report\n",
    "    report = {\n",
    "        'timestamp': timestamp,\n",
    "        'model_path': model_path,\n",
    "        'detection_output_dir': detection_output_dir,\n",
    "        'metrics': metrics_report,\n",
    "        'timing': {\n",
    "            'avg_inference_time_ms': avg_time * 1000 if 'avg_time' in locals() else 'N/A',\n",
    "            'estimated_fps': fps if 'fps' in locals() else 'N/A'\n",
    "        },\n",
    "        'detection_stats': detection_stats if 'detection_stats' in locals() else {},\n",
    "        'threshold_analysis': threshold_results if 'threshold_results' in locals() else {},\n",
    "        'detection_summary': detection_summary if 'detection_summary' in locals() else {}\n",
    "    }\n",
    "    \n",
    "    # Save as JSON\n",
    "    report_path = f\"person_detection/evaluation_report_{timestamp}.json\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2, default=str)\n",
    "    \n",
    "    # 7. Create CLEAN METRICS FILE\n",
    "    print(f\"\\nğŸ“Š 7. CREATING CLEAN METRICS SUMMARY\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    total_val_images = len(list(val_images_dir.glob(\"*.jpg\"))) if val_images_dir.exists() else 0\n",
    "    total_detections = sum(item['total_detections'] for item in detection_summary) if detection_summary else 0\n",
    "    total_high_conf = sum(item['high_confidence'] for item in detection_summary) if detection_summary else 0\n",
    "    \n",
    "    # Create clean metrics file\n",
    "    clean_metrics_path = f\"person_detection/METRICS_SUMMARY_{timestamp}.txt\"\n",
    "    with open(clean_metrics_path, 'w') as f:\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(\"CV AIM ASSIST - MODEL PERFORMANCE METRICS\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Model: {model_path}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        # Core Performance Metrics\n",
    "        f.write(\"ğŸ¯ CORE PERFORMANCE METRICS\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        f.write(f\"mAP@0.5:           {metrics_report['mAP50']:.3f}     (Higher = Better, Max = 1.0)\\n\")\n",
    "        f.write(f\"mAP@0.5:0.95:      {metrics_report['mAP50-95']:.3f}     (Higher = Better, Max = 1.0)\\n\")\n",
    "        f.write(f\"Precision:         {metrics_report['Precision']:.3f}     (Higher = Better, Max = 1.0)\\n\")\n",
    "        f.write(f\"Recall:            {metrics_report['Recall']:.3f}     (Higher = Better, Max = 1.0)\\n\")\n",
    "        f.write(f\"F1-Score:          {metrics_report['F1-Score']:.3f}     (Higher = Better, Max = 1.0)\\n\\n\")\n",
    "        \n",
    "        # Performance Rating\n",
    "        overall_score = (metrics_report['mAP50'] + metrics_report['Precision'] + metrics_report['Recall']) / 3\n",
    "        if overall_score >= 0.8:\n",
    "            rating = \"EXCELLENT â­â­â­â­â­\"\n",
    "        elif overall_score >= 0.7:\n",
    "            rating = \"VERY GOOD â­â­â­â­\"\n",
    "        elif overall_score >= 0.6:\n",
    "            rating = \"GOOD â­â­â­\"\n",
    "        elif overall_score >= 0.5:\n",
    "            rating = \"FAIR â­â­\"\n",
    "        else:\n",
    "            rating = \"NEEDS IMPROVEMENT â­\"\n",
    "        \n",
    "        f.write(f\"ğŸ“ˆ OVERALL RATING\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        f.write(f\"Overall Score:     {overall_score:.3f}     {rating}\\n\\n\")\n",
    "        \n",
    "        # Speed Performance\n",
    "        if 'avg_time' in locals():\n",
    "            f.write(f\"âš¡ SPEED PERFORMANCE\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            f.write(f\"Inference Time:    {avg_time*1000:.1f} ms        (Lower = Better)\\n\")\n",
    "            f.write(f\"Estimated FPS:     {fps:.1f}            (Higher = Better)\\n\")\n",
    "            real_time_status = \"YES âœ…\" if fps >= 30 else \"NO âŒ\"\n",
    "            f.write(f\"Real-time Ready:   {real_time_status}        (30+ FPS needed)\\n\\n\")\n",
    "        \n",
    "        # Detection Analysis\n",
    "        if detection_summary:\n",
    "            avg_detections = total_detections / len(detection_summary)\n",
    "            avg_high_conf = total_high_conf / len(detection_summary)\n",
    "            high_conf_rate = (total_high_conf / total_detections * 100) if total_detections > 0 else 0\n",
    "            \n",
    "            f.write(f\"ğŸ” DETECTION ANALYSIS\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            f.write(f\"Images Tested:     {len(detection_summary)}\\n\")\n",
    "            f.write(f\"Total Detections:  {total_detections}\\n\")\n",
    "            f.write(f\"High Confidence:   {total_high_conf}            (>0.7 threshold)\\n\")\n",
    "            f.write(f\"Avg per Image:     {avg_detections:.1f}            (detections)\\n\")\n",
    "            f.write(f\"High Conf Rate:    {high_conf_rate:.1f}%           (reliability)\\n\\n\")\n",
    "        \n",
    "        # Confidence Thresholds\n",
    "        if 'threshold_results' in locals():\n",
    "            f.write(f\"ğŸšï¸  CONFIDENCE THRESHOLDS\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            for thresh, detections in threshold_results.items():\n",
    "                f.write(f\"Threshold {thresh}:     {detections:.1f} avg detections\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Model Quality Assessment\n",
    "        f.write(f\"ğŸ“‹ MODEL QUALITY ASSESSMENT\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        # Precision assessment\n",
    "        if metrics_report['Precision'] >= 0.8:\n",
    "            f.write(\"Precision:         EXCELLENT - Very few false positives\\n\")\n",
    "        elif metrics_report['Precision'] >= 0.7:\n",
    "            f.write(\"Precision:         GOOD - Some false positives\\n\")\n",
    "        elif metrics_report['Precision'] >= 0.6:\n",
    "            f.write(\"Precision:         FAIR - Moderate false positives\\n\")\n",
    "        else:\n",
    "            f.write(\"Precision:         POOR - Too many false positives\\n\")\n",
    "        \n",
    "        # Recall assessment\n",
    "        if metrics_report['Recall'] >= 0.8:\n",
    "            f.write(\"Recall:            EXCELLENT - Finds most characters\\n\")\n",
    "        elif metrics_report['Recall'] >= 0.7:\n",
    "            f.write(\"Recall:            GOOD - Finds many characters\\n\")\n",
    "        elif metrics_report['Recall'] >= 0.6:\n",
    "            f.write(\"Recall:            FAIR - Misses some characters\\n\")\n",
    "        else:\n",
    "            f.write(\"Recall:            POOR - Misses too many characters\\n\")\n",
    "        \n",
    "        # mAP assessment\n",
    "        if metrics_report['mAP50'] >= 0.8:\n",
    "            f.write(\"mAP@0.5:           EXCELLENT - Very accurate bounding boxes\\n\")\n",
    "        elif metrics_report['mAP50'] >= 0.7:\n",
    "            f.write(\"mAP@0.5:           GOOD - Accurate bounding boxes\\n\")\n",
    "        elif metrics_report['mAP50'] >= 0.6:\n",
    "            f.write(\"mAP@0.5:           FAIR - Somewhat accurate boxes\\n\")\n",
    "        else:\n",
    "            f.write(\"mAP@0.5:           POOR - Inaccurate bounding boxes\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Recommendations\n",
    "        f.write(f\"ğŸ’¡ RECOMMENDATIONS\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        if overall_score >= 0.7:\n",
    "            f.write(\"âœ… Model is ready for production use!\\n\")\n",
    "            f.write(\"âœ… Good balance of precision and recall\\n\")\n",
    "            if 'fps' in locals() and fps >= 30:\n",
    "                f.write(\"âœ… Fast enough for real-time gaming\\n\")\n",
    "            else:\n",
    "                f.write(\"âš ï¸  Consider using smaller model for better speed\\n\")\n",
    "        elif overall_score >= 0.6:\n",
    "            f.write(\"âš ï¸  Model is decent but could be improved\\n\")\n",
    "            if metrics_report['Precision'] < 0.7:\n",
    "                f.write(\"ğŸ“ˆ Consider: More training data to reduce false positives\\n\")\n",
    "            if metrics_report['Recall'] < 0.7:\n",
    "                f.write(\"ğŸ“ˆ Consider: Lower confidence threshold or more training\\n\")\n",
    "        else:\n",
    "            f.write(\"âŒ Model needs significant improvement\\n\")\n",
    "            f.write(\"ğŸ“ˆ Suggested actions:\\n\")\n",
    "            f.write(\"   - Collect more diverse training data\\n\")\n",
    "            f.write(\"   - Train for more epochs\\n\")\n",
    "            f.write(\"   - Check data quality and labeling\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(\"For detailed analysis, check the visual outputs in:\\n\")\n",
    "        f.write(f\"{detection_output_dir}/\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    # Save as readable text (detailed version)\n",
    "    txt_report_path = f\"person_detection/evaluation_report_{timestamp}.txt\"\n",
    "    with open(txt_report_path, 'w') as f:\n",
    "        f.write(\"CV AIM ASSIST MODEL EVALUATION REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(f\"Generated: {timestamp}\\n\")\n",
    "        f.write(f\"Model: {model_path}\\n\")\n",
    "        f.write(f\"Detection Visualizations: {detection_output_dir}/\\n\\n\")\n",
    "        \n",
    "        f.write(\"VALIDATION METRICS:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for key, value in metrics_report.items():\n",
    "            f.write(f\"{key}: {value:.3f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nPERFORMANCE:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        if 'avg_time' in locals():\n",
    "            f.write(f\"Inference Time: {avg_time*1000:.1f}ms\\n\")\n",
    "            f.write(f\"Estimated FPS: {fps:.1f}\\n\")\n",
    "            f.write(f\"Real-time Ready: {'Yes' if fps >= 30 else 'No'}\\n\")\n",
    "        \n",
    "        if 'detection_stats' in locals():\n",
    "            f.write(f\"\\nDETECTION ANALYSIS:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"Total Detections: {detection_stats['total_detections']}\\n\")\n",
    "            f.write(f\"High Confidence: {detection_stats['high_conf_detections']}\\n\")\n",
    "        \n",
    "        if 'threshold_results' in locals():\n",
    "            f.write(f\"\\nCONFIDENCE THRESHOLDS:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            for thresh, detections in threshold_results.items():\n",
    "                f.write(f\"Threshold {thresh}: {detections:.1f} avg detections\\n\")\n",
    "    \n",
    "    print(f\"ğŸ“„ JSON report saved: {report_path}\")\n",
    "    print(f\"ğŸ“„ Detailed report saved: {txt_report_path}\")\n",
    "    print(f\"ğŸ“Š CLEAN METRICS saved: {clean_metrics_path}\")\n",
    "    print(f\"ğŸ¨ Visual outputs saved: {detection_output_dir}/\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ EVALUATION COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Return summary for further use\n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': metrics_report,\n",
    "        'report_path': report_path,\n",
    "        'detection_output_dir': detection_output_dir\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea33543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ COMPLETE TRAINING PIPELINE\n",
      "============================================================\n",
      "ğŸ“‹ Pipeline Steps:\n",
      "   1. Split data into train/validation (80%/20%)\n",
      "   2. Fine-tune YOLO person detection\n",
      "   3. Comprehensive model evaluation\n",
      "   4. Save detailed performance report\n",
      "============================================================\n",
      "ğŸ”¥ STEP 1-2: Training with data splitting...\n",
      "ğŸ”„ Preparing dataset...\n",
      "ğŸ“Š Splitting dataset into train/validation sets...\n",
      "Found 7670 images to split\n",
      "ğŸ“ Training set: 6136 images (80.0%)\n",
      "ğŸ“ Validation set: 1534 images (20.0%)\n",
      "âœ… Dataset split completed!\n",
      "ğŸ“ Created dataset_split.yaml with train/val paths\n",
      "\n",
      "============================================================\n",
      "YOLOv8 Person Detection Fine-tuning (Game Characters)\n",
      "============================================================\n",
      "ğŸ“¦ Loading pretrained model: yolov8n.pt\n",
      "ğŸ“Š Model architecture: yolov8n.pt\n",
      "ğŸ“ Dataset: augmented_data/dataset_split.yaml\n",
      "ğŸ¯ Target: Person detection (leveraging pretrained class)\n",
      "âš™ï¸  Epochs: 25 (reduced due to transfer learning)\n",
      "ğŸ“ Image size: 640\n",
      "ğŸ”¢ Batch size: 16\n",
      "ğŸ’¡ Strategy: Game characters â†’ Person class (ID 0)\n",
      "ğŸ“Š Data split: 80% train, 20% validation\n",
      "\n",
      "ğŸš€ Starting fine-tuning...\n",
      "New https://pypi.org/project/ultralytics/8.3.184 available ğŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.179 ğŸš€ Python-3.9.23 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 7815MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=augmented_data/dataset_split.yaml, degrees=5.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=25, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.01, hsv_s=0.4, hsv_v=0.3, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.1, mask_ratio=4, max_det=300, mixup=0.1, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.8, multi_scale=False, name=game_characters, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=15, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=person_detection, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=person_detection/game_characters, save_frames=False, save_json=False, save_period=5, save_txt=False, scale=0.2, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.05, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 5210.4Â±1288.7 MB/s, size: 309.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/kronbii/repos/CV-aim-assist/augmented_data/train/labels... 6136 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6136/6136 [00:02<00:00, 2327.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/kronbii/repos/CV-aim-assist/augmented_data/train/labels.cache\n",
      "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (3.9GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6136/6136 [00:08<00:00, 720.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1688.0Â±1161.3 MB/s, size: 271.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kronbii/repos/CV-aim-assist/augmented_data/val/labels... 1534 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1534/1534 [00:01<00:00, 1209.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/kronbii/repos/CV-aim-assist/augmented_data/val/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (1.0GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1534/1534 [00:02<00:00, 685.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to person_detection/game_characters/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mperson_detection/game_characters\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/25      2.12G      1.105      1.049      1.112         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:52<00:00,  7.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:06<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.992      0.991      0.995      0.772\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/25      2.14G     0.9115     0.6217      1.023         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:48<00:00,  7.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.995      0.991      0.995      0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/25      2.16G     0.8878     0.5626      1.023         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:48<00:00,  7.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:06<00:00,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.987      0.982      0.994      0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/25      2.17G     0.8522     0.5331       1.01         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:50<00:00,  7.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.997      0.997      0.995      0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/25      2.19G     0.8178     0.5013     0.9983         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:50<00:00,  7.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.996      0.991      0.995      0.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/25      2.21G     0.7947     0.4784     0.9885         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:51<00:00,  7.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.998      0.985      0.995      0.827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/25      2.22G      0.772      0.464     0.9829         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:51<00:00,  7.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:06<00:00,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.997      0.997      0.995      0.845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/25      2.24G     0.7546     0.4481      0.972         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:51<00:00,  7.45it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.997      0.996      0.995      0.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/25      2.26G      0.745     0.4397     0.9706         43        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:50<00:00,  7.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.998      0.997      0.995      0.847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/25      2.28G      0.728      0.428      0.965         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:52<00:00,  7.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:06<00:00,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.998      0.997      0.995      0.844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/25      2.29G      0.712     0.4153     0.9583         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:50<00:00,  7.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.998      0.998      0.995      0.842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/25      2.31G     0.7145     0.4157     0.9624         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:51<00:00,  7.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.998      0.999      0.995      0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/25      2.33G     0.6898     0.3989     0.9519         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:50<00:00,  7.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.999      0.999      0.995      0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/25      2.34G     0.6776      0.387     0.9454         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:51<00:00,  7.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.998      0.998      0.995      0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/25      2.36G     0.6758     0.3867     0.9427         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:50<00:00,  7.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.998      0.999      0.995      0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/25      2.38G     0.5611     0.2568     0.8727         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:51<00:00,  7.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:06<00:00,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.999      0.998      0.995      0.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/25      2.39G     0.5517     0.2488     0.8716         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:51<00:00,  7.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.998      0.999      0.995       0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/25      2.41G     0.5412     0.2418      0.866         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:50<00:00,  7.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.999      0.998      0.995      0.887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/25      2.43G     0.5314     0.2366     0.8622         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:50<00:00,  7.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.999          1      0.995      0.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/25      2.45G     0.5209     0.2295     0.8597         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:51<00:00,  7.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.998      0.999      0.995      0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/25      2.47G     0.5141     0.2263     0.8546         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:51<00:00,  7.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.999      0.999      0.995      0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/25      2.48G     0.5056     0.2213     0.8563         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:51<00:00,  7.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:06<00:00,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.999      0.999      0.995      0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/25       2.5G     0.4944     0.2144     0.8535         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:51<00:00,  7.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.999          1      0.995      0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/25      2.51G      0.486     0.2076     0.8487         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:50<00:00,  7.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764          1          1      0.995      0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/25      2.53G     0.4795     0.2066     0.8479         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:50<00:00,  7.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:05<00:00,  8.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.999          1      0.995      0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "25 epochs completed in 0.397 hours.\n",
      "Optimizer stripped from person_detection/game_characters/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from person_detection/game_characters/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating person_detection/game_characters/weights/best.pt...\n",
      "Ultralytics 8.3.179 ğŸš€ Python-3.9.23 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 7815MiB)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:06<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1534       3764      0.999          1      0.995      0.897\n",
      "Speed: 0.2ms preprocess, 0.9ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mperson_detection/game_characters\u001b[0m\n",
      "\n",
      "âœ… Fine-tuning completed!\n",
      "ğŸ“ Results saved to: person_detection/game_characters\n",
      "ğŸ† Best model: person_detection/game_characters/weights/best.pt\n",
      "\n",
      "ğŸ” Running validation on held-out validation set...\n",
      "Ultralytics 8.3.179 ğŸš€ Python-3.9.23 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 7815MiB)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 347.4Â±55.5 MB/s, size: 267.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kronbii/repos/CV-aim-assist/augmented_data/val/labels.cache... 1534 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1534/1534 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ \u001b[34m\u001b[1mval: \u001b[0m1.5GB RAM required to cache images with 50% safety margin but only 1.1/15.5GB available, not caching images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   2%|â–         | 2/96 [00:01<01:04,  1.45it/s]"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ EXECUTE: Complete Training Pipeline with Evaluation\n",
    "\n",
    "print(\"ğŸš€ COMPLETE TRAINING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“‹ Pipeline Steps:\")\n",
    "print(\"   1. Split data into train/validation (80%/20%)\")\n",
    "print(\"   2. Fine-tune YOLO person detection\")\n",
    "print(\"   3. Comprehensive model evaluation\")\n",
    "print(\"   4. Save detailed performance report\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1 & 2: Train the model (includes data splitting)\n",
    "try:\n",
    "    print(\"ğŸ”¥ STEP 1-2: Training with data splitting...\")\n",
    "    model, results = train_person_detection_model()\n",
    "    \n",
    "    if model is not None:\n",
    "        print(\"\\n\" + \"ğŸ‰\" * 20)\n",
    "        print(\"TRAINING SUCCESS!\")\n",
    "        print(\"ğŸ‰\" * 20)\n",
    "        \n",
    "        # Step 3: Comprehensive evaluation\n",
    "        print(f\"\\nğŸ” STEP 3: Comprehensive Model Evaluation...\")\n",
    "        print(\"This will test the model on validation data and save detailed reports\")\n",
    "        \n",
    "        evaluation_results = comprehensive_model_evaluation()\n",
    "        \n",
    "        if evaluation_results:\n",
    "            print(\"\\n\" + \"âœ…\" * 20)\n",
    "            print(\"EVALUATION COMPLETE!\")\n",
    "            print(\"âœ…\" * 20)\n",
    "            \n",
    "            print(f\"\\nğŸ“Š FINAL PERFORMANCE SUMMARY:\")\n",
    "            print(\"-\" * 40)\n",
    "            metrics = evaluation_results['metrics']\n",
    "            print(f\"ğŸ¯ mAP@0.5: {metrics['mAP50']:.3f}\")\n",
    "            print(f\"ğŸ¯ Precision: {metrics['Precision']:.3f}\")\n",
    "            print(f\"ğŸ¯ Recall: {metrics['Recall']:.3f}\")\n",
    "            print(f\"ğŸ¯ F1-Score: {metrics['F1-Score']:.3f}\")\n",
    "            \n",
    "            print(f\"\\nğŸ“ Files Generated:\")\n",
    "            print(f\"   ğŸ† Best Model: person_detection/game_characters/weights/best.pt\")\n",
    "            print(f\"   ğŸ“Š Training Plots: person_detection/game_characters/\")\n",
    "            print(f\"   ï¿½ Evaluation Report: {evaluation_results['report_path']}\")\n",
    "            \n",
    "            print(f\"\\nğŸ® READY FOR AIM ASSIST!\")\n",
    "            print(\"Your model is trained and evaluated. Key takeaways:\")\n",
    "            print(\"âœ… Detects game characters as 'person' class\")\n",
    "            print(\"âœ… Leverages pretrained human detection knowledge\")\n",
    "            print(\"âœ… Fast inference for real-time gaming\")\n",
    "            print(\"âœ… Detailed performance metrics saved\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ Evaluation failed - but training was successful!\")\n",
    "            print(\"ğŸ’¡ You can still use the trained model\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âŒ Training failed!\")\n",
    "        print(\"ğŸ’¡ Make sure you've run the data augmentation notebook first\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Pipeline failed: {e}\")\n",
    "    print(\"ğŸ’¡ Check that you've run data augmentation and have enough data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ NEXT STEPS:\")\n",
    "print(\"1. Review the evaluation report for detailed metrics\")\n",
    "print(\"2. Check training plots in person_detection/game_characters/\")\n",
    "print(\"3. Use best.pt model for real-time aim assist\")\n",
    "print(\"4. Test the model on new game footage\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fortnite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
